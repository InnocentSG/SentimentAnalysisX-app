# -*- coding: utf-8 -*-
"""twitter_sentiment_analysis(github code).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Pr7oMhPzDu9u14YRFSQ-fPgBK49QR2iF
"""
import streamlit as st
import re
import numpy as np
import pandas as pd
import string
import nltk
import gdown
import os
import tweepy
from nltk.tokenize import RegexpTokenizer
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.corpus import stopwords
import seaborn as sns
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import BernoulliNB
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import (confusion_matrix,
                           classification_report,
                           accuracy_score)

# Download NLTK resources
nltk.download('wordnet')
nltk.download('stopwords')

# Constants
DATASET_COLUMNS = ['target', 'ids', 'date', 'flag', 'user', 'text']
DATASET_ENCODING = "ISO-8859-1"
SAMPLE_SIZE = 20000
GOOGLE_DRIVE_FILE_ID = "1zUJPJM11P3AoLUmAzLS7-xAKMERSG-GV"
DATASET_URL = f"https://drive.google.com/uc?id={GOOGLE_DRIVE_FILE_ID}"
DATASET_PATH = "data/twitter_dataset_1.6M.csv"

# Twitter API credentials (optional)
TWITTER_CONSUMER_KEY = st.secrets.get("TWITTER_CONSUMER_KEY", "")
TWITTER_CONSUMER_SECRET = st.secrets.get("TWITTER_CONSUMER_SECRET", "")
TWITTER_ACCESS_TOKEN = st.secrets.get("TWITTER_ACCESS_TOKEN", "")
TWITTER_ACCESS_TOKEN_SECRET = st.secrets.get("TWITTER_ACCESS_TOKEN_SECRET", "")

# Set page config
st.set_page_config(
    page_title="Twitter Sentiment Analysis",
    page_icon="X",
    layout="wide"
)

@st.cache_data
def load_dataset():
    """Load and prepare the Twitter sentiment dataset"""
    os.makedirs("data", exist_ok=True)
    
    if not os.path.exists(DATASET_PATH):
        with st.spinner('Downloading dataset...'):
            try:
                gdown.download(DATASET_URL, DATASET_PATH, quiet=False)
            except Exception as e:
                st.error(f"Failed to download dataset: {e}")
                st.stop()
    
    try:
        df = pd.read_csv(DATASET_PATH,
                        encoding=DATASET_ENCODING,
                        names=DATASET_COLUMNS)
        df['target'] = df['target'].replace(4, 2)  # Convert to 0=neg, 1=neutral, 2=pos
        pos_data = df[df['target'] == 2].sample(SAMPLE_SIZE, random_state=42)
        neg_data = df[df['target'] == 0].sample(SAMPLE_SIZE, random_state=42)
        neutral_data = df.sample(SAMPLE_SIZE, random_state=42)
        neutral_data['target'] = 1
        return pd.concat([pos_data, neg_data, neutral_data])
    except Exception as e:
        st.error(f"Error loading dataset: {e}")
        st.stop()

class TextPreprocessor:
    def __init__(self):
        self.stopwords = set(stopwords.words('english'))
        self.stemmer = PorterStemmer()
        self.lemmatizer = WordNetLemmatizer()
        self.tokenizer = RegexpTokenizer(r'\w+')

    def clean_text(self, text):
        text = str(text).lower()
        text = re.sub('((www.[^\s]+)|(https?://[^\s]+))', ' ', text)
        text = text.translate(str.maketrans('', '', string.punctuation))
        text = re.sub('[0-9]+', '', text)
        text = ' '.join([word for word in text.split() if word not in self.stopwords])
        text = re.sub(r'(.)\1+', r'\1', text)
        tokens = self.tokenizer.tokenize(text)
        tokens = [self.stemmer.stem(token) for token in tokens]
        tokens = [self.lemmatizer.lemmatize(token) for token in tokens]
        return ' '.join(tokens)

def plot_wordcloud(text, title):
    wc = WordCloud(max_words=1000, width=1600, height=800,
                  collocations=False).generate(text)
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.imshow(wc)
    ax.set_title(title, fontsize=20)
    ax.axis('off')
    st.pyplot(fig)

def plot_confusion_matrix(y_true, y_pred, classes):
    """Fixed version for multi-class classification"""
    cm = confusion_matrix(y_true, y_pred)
    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    fig, ax = plt.subplots(figsize=(8, 6))
    sns.heatmap(
        cm_percent,
        annot=True,
        fmt=".2%",
        cmap="Blues",
        xticklabels=classes,
        yticklabels=classes,
        ax=ax
    )
    ax.set_xlabel('Predicted', fontsize=14)
    ax.set_ylabel('Actual', fontsize=14)
    ax.set_title('Confusion Matrix (Normalized)', fontsize=18)
    st.pyplot(fig)

def evaluate_model(model, X_test, y_test, model_name):
    y_pred = model.predict(X_test)

    st.subheader(f"{model_name} Performance")
    st.write(f"Accuracy: {accuracy_score(y_test, y_pred):.2%}")
    
    st.write("Classification Report:")
    report = classification_report(y_test, y_pred, output_dict=True)
    st.table(pd.DataFrame(report).transpose())

    plot_confusion_matrix(y_test, y_pred, ['Negative', 'Neutral', 'Positive'])

@st.cache_resource
def get_vectorizer(text_data):
    """Cache the vectorizer to avoid refitting"""
    vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=100000)
    return vectorizer.fit(text_data)

@st.cache_resource
def train_models(model_choices, X_train_vec, y_train):
    """Train and cache selected models"""
    models = {}
    if "Naive Bayes" in model_choices:
        models["Naive Bayes"] = BernoulliNB().fit(X_train_vec, y_train)
    if "Linear SVC" in model_choices:
        models["Linear SVC"] = LinearSVC(max_iter=1000, random_state=42).fit(X_train_vec, y_train)
    if "Logistic Regression" in model_choices:
        models["Logistic Regression"] = LogisticRegression(max_iter=1000, random_state=42).fit(X_train_vec, y_train)
    return models

def get_twitter_client():
    if not all([TWITTER_CONSUMER_KEY, TWITTER_CONSUMER_SECRET, 
               TWITTER_ACCESS_TOKEN, TWITTER_ACCESS_TOKEN_SECRET]):
        return None
    try:
        auth = tweepy.OAuthHandler(TWITTER_CONSUMER_KEY, TWITTER_CONSUMER_SECRET)
        auth.set_access_token(TWITTER_ACCESS_TOKEN, TWITTER_ACCESS_TOKEN_SECRET)
        return tweepy.API(auth, wait_on_rate_limit=True)
    except Exception as e:
        st.warning(f"Twitter API failed: {e}")
        return None

def predict_twitter_link(tweet_url, vectorizer, models, preprocessor):
    tweet_id = re.search(r'status/(\d+)', tweet_url)
    if not tweet_id:
        st.error("Invalid Twitter URL")
        return None
    
    twitter_client = get_twitter_client()
    if not twitter_client:
        st.error("Twitter API not configured")
        return None
    
    try:
        tweet = twitter_client.get_status(tweet_id.group(1), tweet_mode='extended')
        text = tweet.full_text
        st.write("### Original Tweet:")
        st.write(text)
        
        cleaned_text = preprocessor.clean_text(text)
        input_vec = vectorizer.transform([cleaned_text])
        
        results = {}
        for name, model in models.items():
            prediction = model.predict(input_vec)[0]
            sentiment = "Positive" if prediction == 2 else "Negative" if prediction == 0 else "Neutral"
            results[name] = sentiment
        
        st.subheader("Prediction Results")
        for model_name, sentiment in results.items():
            st.metric(label=model_name, value=sentiment)
    except Exception as e:
        st.error(f"Error fetching tweet: {e}")

def main():
    st.title("üê¶ Twitter Sentiment Analysis")
    
    # Load data
    df = load_dataset()
    
    # EDA
    st.header("Data Overview")
    st.write(f"Dataset shape: {df.shape}")
    
    fig, ax = plt.subplots()
    sns.countplot(x='target', data=df, ax=ax)
    ax.set_title('Class Distribution')
    st.pyplot(fig)
    
    # Preprocessing
    with st.spinner('Preprocessing text...'):
        preprocessor = TextPreprocessor()
        df['cleaned_text'] = df['text'].apply(preprocessor.clean_text)
    
    # Word Clouds
    st.header("Word Clouds")
    col1, col2, col3 = st.columns(3)
    with col1:
        plot_wordcloud(' '.join(df[df['target']==2]['cleaned_text']), "Positive")
    with col2:
        plot_wordcloud(' '.join(df[df['target']==1]['cleaned_text']), "Neutral")
    with col3:
        plot_wordcloud(' '.join(df[df['target']==0]['cleaned_text']), "Negative")
    
    # Model Training
    st.header("Model Training")
    X_train, X_test, y_train, y_test = train_test_split(
        df['cleaned_text'], df['target'], test_size=0.2, random_state=42)
    
    with st.spinner('Vectorizing text...'):
        vectorizer = get_vectorizer(X_train)
        X_train_vec = vectorizer.transform(X_train)
        X_test_vec = vectorizer.transform(X_test)
    
    # Model selection
    st.sidebar.header("Models")
    model_choices = []
    if st.sidebar.checkbox("Naive Bayes", True, key='nb'):
        model_choices.append("Naive Bayes")
    if st.sidebar.checkbox("Linear SVC", True, key='svc'):
        model_choices.append("Linear SVC")
    if st.sidebar.checkbox("Logistic Regression", True, key='lr'):
        model_choices.append("Logistic Regression")
    
    # Train and evaluate models (cached)
    if model_choices:
        models = train_models(model_choices, X_train_vec, y_train)
        for name, model in models.items():
            evaluate_model(model, X_test_vec, y_test, name)
    
    # Prediction
    st.header("Live Prediction")
    option = st.radio("Input type:", ("Text", "Twitter URL"), key='input_type')
    
    if option == "Text":
        text_input = st.text_area("Enter text to analyze:", key='text_input')
        if text_input and 'models' in locals():
            cleaned_input = preprocessor.clean_text(text_input)
            input_vec = vectorizer.transform([cleaned_input])
            results = {name: model.predict(input_vec)[0] for name, model in models.items()}
            st.subheader("Results")
            for name, pred in results.items():
                st.write(f"{name}: {'Positive' if pred == 2 else 'Negative' if pred == 0 else 'Neutral'}")
    else:
        url_input = st.text_input("Enter Tweet URL:", key='tweet_url')
        if url_input and 'models' in locals():
            predict_twitter_link(url_input, vectorizer, models, preprocessor)

if __name__ == "__main__":
    main()
